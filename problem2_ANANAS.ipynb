{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8f9afd",
   "metadata": {},
   "source": [
    "# I. Info\n",
    "\n",
    "Tên đội thi: ANANAS\n",
    "- Mã Thuỳ Anh\n",
    "- Nhan Hữu Hiếu\n",
    "- Trương Thành Nhân"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7251d6f5",
   "metadata": {},
   "source": [
    "# II. Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b0528b",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21704390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://fiinquant.github.io/fiinquantx/simple\n",
      "Requirement already satisfied: fiinquantx in /opt/anaconda3/lib/python3.12/site-packages (0.1.36)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (2.3.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (2.2.6)\n",
      "Requirement already satisfied: signalrcore in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (0.9.5)\n",
      "Requirement already satisfied: fastdtw in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (0.3.4)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (3.9.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (1.13.1)\n",
      "Requirement already satisfied: python_dotenv in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (0.21.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (1.5.1)\n",
      "Requirement already satisfied: plotly in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (6.1.1)\n",
      "Requirement already satisfied: stumpy in /opt/anaconda3/lib/python3.12/site-packages (from fiinquantx) (1.13.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->fiinquantx) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->fiinquantx) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->fiinquantx) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->fiinquantx) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->fiinquantx) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->fiinquantx) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->fiinquantx) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->fiinquantx) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->fiinquantx) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->fiinquantx) (2023.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/anaconda3/lib/python3.12/site-packages (from plotly->fiinquantx) (1.48.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->fiinquantx) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->fiinquantx) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->fiinquantx) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->fiinquantx) (2025.6.15)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->fiinquantx) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->fiinquantx) (3.5.0)\n",
      "Requirement already satisfied: websocket-client==1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from signalrcore->fiinquantx) (1.0.0)\n",
      "Requirement already satisfied: msgpack==1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from signalrcore->fiinquantx) (1.0.2)\n",
      "Requirement already satisfied: numba>=0.57.1 in /opt/anaconda3/lib/python3.12/site-packages (from stumpy->fiinquantx) (0.61.2)\n",
      "Collecting numpy (from fiinquantx)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/anaconda3/lib/python3.12/site-packages (from numba>=0.57.1->stumpy->fiinquantx) (0.44.0)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-ta 0.4.67b0 requires numpy>=2.2.6, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: optuna in /opt/anaconda3/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: pandas_ta in /opt/anaconda3/lib/python3.12/site-packages (0.4.67b0)\n",
      "Requirement already satisfied: lightgbm in /opt/anaconda3/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (1.13.3)\n",
      "Requirement already satisfied: colorlog in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (1.4.54)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: numba==0.61.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_ta) (0.61.2)\n",
      "Collecting numpy (from optuna)\n",
      "  Using cached numpy-2.3.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: pandas>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_ta) (2.3.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/anaconda3/lib/python3.12/site-packages (from numba==0.61.2->pandas_ta) (0.44.0)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.13.1)\n",
      "Requirement already satisfied: Mako in /opt/anaconda3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/anaconda3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.3.2->pandas_ta) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.3.2->pandas_ta) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.3.2->pandas_ta) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.3.2->pandas_ta) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/anaconda3/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Using cached numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --extra-index-url https://fiinquant.github.io/fiinquantx/simple fiinquantx\n",
    "%pip install optuna pandas_ta lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thư viện chuẩn của Python\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Thư viện của bên thứ ba\n",
    "import FiinQuantX as fq\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import (classification_report, f1_score,\n",
    "                             precision_recall_curve, roc_auc_score)\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c21d34",
   "metadata": {},
   "source": [
    "## 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f89288fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in...\n",
      "Login successful!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "username = os.getenv(\"API_USERNAME\")\n",
    "password = os.getenv(\"API_PASSWORD\")\n",
    "\n",
    "try:\n",
    "    print(\"Logging in...\")\n",
    "    client = fq.FiinSession(username=username, password=password).login()\n",
    "    print(\"Login successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Login error: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a73f26",
   "metadata": {},
   "source": [
    "# III. Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5aa2c8",
   "metadata": {},
   "source": [
    "## 1. Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ticker_list(client, index_code, filename, output_dir=\"ticker_lists\"):\n",
    "    \"\"\"Fetch a ticker list for a given index and save it as CSV.\"\"\"\n",
    "    tickers = client.TickerList(ticker=index_code)\n",
    "    print(f\"{index_code} has {len(tickers)} tickers. Example: {tickers[:5]}\")\n",
    "    \n",
    "    df = pd.DataFrame(tickers, columns=['ticker'])\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "    print(f\"   Saved to file: {filepath}\")\n",
    "\n",
    "OUTPUT_DIR = \"ticker_lists\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    print(\"\\nFetching ticker lists using TickerList...\")\n",
    "    exchanges = [\n",
    "        (\"VNINDEX\", \"hose_tickers.csv\"),\n",
    "        (\"HNXINDEX\", \"hnx_tickers.csv\"),\n",
    "        (\"UPCOMINDEX\", \"upcom_tickers.csv\")\n",
    "    ]\n",
    "    \n",
    "    for index_code, filename in exchanges:\n",
    "        save_ticker_list(client, index_code, filename, OUTPUT_DIR)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error while fetching ticker lists: {e}\")\n",
    "\n",
    "print(f\"\\nProcess completed! All CSV files saved in '{OUTPUT_DIR}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b255de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = fq.FiinSession(username=username, password=password).login()\n",
    "\n",
    "def fetch_range_batch(tickers, start, end):\n",
    "    try:\n",
    "        df = client.Fetch_Trading_Data(\n",
    "            realtime=False,\n",
    "            tickers=tickers,\n",
    "            fields=[\"open\", \"high\", \"low\", \"close\", \"volume\"],\n",
    "            adjusted=True,\n",
    "            by=\"1d\",\n",
    "            from_date=start.strftime(\"%Y-%m-%d\"),\n",
    "            to_date=end.strftime(\"%Y-%m-%d\")\n",
    "        ).get_data()\n",
    "        return pd.DataFrame(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {tickers[:3]}...: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def fetch_exchange_fast(exchange, file, batch_size=100, max_workers=5):\n",
    "    tickers = pd.read_csv(file).iloc[:,0].dropna().tolist()\n",
    "    all_data = []\n",
    "    today = datetime.today()\n",
    "    start = today - timedelta(days=365*3)\n",
    "\n",
    "    while start < today:\n",
    "        end = min(start + timedelta(days=90), today)\n",
    "        print(f\"\\nFetching {exchange}: {start.date()} → {end.date()} ({len(tickers)} tickers)\")\n",
    "\n",
    "        batches = [tickers[i:i+batch_size] for i in range(0, len(tickers), batch_size)]\n",
    "\n",
    "        futures = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for batch in batches:\n",
    "                futures.append(executor.submit(fetch_range_batch, batch, start, end))\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                df = future.result()\n",
    "                if not df.empty:\n",
    "                    all_data.append(df)\n",
    "\n",
    "        start = end + timedelta(days=1)\n",
    "\n",
    "    if all_data:\n",
    "        df_all = pd.concat(all_data, ignore_index=True)\n",
    "        out_path = f\"{exchange.lower()}_all_by1d.parquet\"\n",
    "        df_all.to_parquet(out_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "        print(f\"Saved {out_path}, {len(df_all)} rows, {df_all['ticker'].nunique()} tickers\")\n",
    "    else:\n",
    "        print(f\"No data was fetched for the {exchange} exchange\")\n",
    "\n",
    "fetch_exchange_fast(\"HOSE\", \"ticker_lists/hose_tickers.csv\")\n",
    "fetch_exchange_fast(\"HNX\", \"ticker_lists/hnx_tickers.csv\")\n",
    "fetch_exchange_fast(\"UPCOM\", \"ticker_lists/upcom_tickers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cdede1",
   "metadata": {},
   "source": [
    "## 2. Calc FA - TA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d31657",
   "metadata": {},
   "source": [
    "### 2.1 FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8014b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_ta(func, *args, **kwargs):\n",
    "    try:\n",
    "        res = func(*args, **kwargs)\n",
    "        if res is None:\n",
    "            return pd.Series([np.nan]*len(args[0]))\n",
    "        if isinstance(res, pd.DataFrame) and res.empty:\n",
    "            return pd.DataFrame(np.nan, index=args[0].index, columns=res.columns)\n",
    "        return res\n",
    "    except Exception:\n",
    "        return pd.Series([np.nan]*len(args[0]))\n",
    "\n",
    "def add_all_indicators_with_patterns(group):\n",
    "    group = group.set_index(\"timestamp\").sort_index()\n",
    "    \n",
    "    close = group[\"close\"].astype(float)\n",
    "    open_ = group[\"open\"].astype(float)\n",
    "    high = group[\"high\"].astype(float)\n",
    "    low = group[\"low\"].astype(float)\n",
    "    volume = group[\"volume\"].astype(float)\n",
    "\n",
    "    if len(close) >= 20: group[\"MA20\"] = safe_ta(ta.sma, close, length=20)\n",
    "    if len(close) >= 50: group[\"MA50\"] = safe_ta(ta.sma, close, length=50)\n",
    "    if len(close) >= 200: group[\"MA200\"] = safe_ta(ta.sma, close, length=200)\n",
    "\n",
    "    if len(close) >= 35:\n",
    "        macd = safe_ta(ta.macd, close, fast=12, slow=26, signal=9)\n",
    "        if isinstance(macd, pd.DataFrame):\n",
    "            group[\"MACD\"] = macd.get(\"MACD_12_26_9\")\n",
    "            group[\"MACD_signal\"] = macd.get(\"MACDs_12_26_9\")\n",
    "            group[\"MACD_hist\"] = macd.get(\"MACDh_12_26_9\")\n",
    "\n",
    "    adx = safe_ta(ta.adx, high, low, close, length=14)\n",
    "    if isinstance(adx, pd.DataFrame):\n",
    "        group[\"ADX\"] = adx.get(\"ADX_14\")\n",
    "        group['DMP'] = adx.get('DMP_14')\n",
    "        group['DMN'] = adx.get('DMN_14')\n",
    "\n",
    "    group[\"RSI14\"] = safe_ta(ta.rsi, close, length=14)\n",
    "    stoch = safe_ta(ta.stoch, high, low, close, k=14, d=3, smooth_k=3)\n",
    "    if isinstance(stoch, pd.DataFrame):\n",
    "        group[\"Stoch_K\"] = stoch.get(\"STOCHk_14_3_3\")\n",
    "        group[\"Stoch_D\"] = stoch.get(\"STOCHd_14_3_3\")\n",
    "\n",
    "    bbands = safe_ta(ta.bbands, close, length=20, std=2)\n",
    "    if isinstance(bbands, pd.DataFrame):\n",
    "        group[\"BB_upper\"] = bbands.get(\"BBU_20_2.0\")\n",
    "        group[\"BB_middle\"] = bbands.get(\"BBM_20_2.0\")\n",
    "        group[\"BB_lower\"] = bbands.get(\"BBL_20_2.0\")\n",
    "    group[\"ATR14\"] = safe_ta(ta.atr, high, low, close, length=14)\n",
    "\n",
    "    group[\"MFI\"] = safe_ta(ta.mfi, high, low, close, volume, length=14)\n",
    "    group[\"OBV\"] = safe_ta(ta.obv, close, volume)\n",
    "    group[\"VWAP\"] = safe_ta(ta.vwap, high, low, close, volume)\n",
    "\n",
    "    group[\"Prev_Close\"] = close.shift(1)\n",
    "    group[\"Gap\"] = ((open_ - group[\"Prev_Close\"].fillna(open_)) / group[\"Prev_Close\"].fillna(open_)) * 100\n",
    "\n",
    "    group[\"Doji\"] = (abs(close - open_) <= (high - low) * 0.1).astype(int)\n",
    "    group[\"Hammer\"] = (\n",
    "        ((close > open_) & ((open_ - low) >= 2 * abs(close - open_))) |\n",
    "        ((open_ > close) & ((close - low) >= 2 * abs(close - open_)))\n",
    "    ).astype(int)\n",
    "\n",
    "    prev_open, prev_close = open_.shift(1), close.shift(1)\n",
    "    group[\"Bullish_Engulfing\"] = (\n",
    "        (prev_close < prev_open) & (close > open_) & (close > prev_open) & (open_ < prev_close)\n",
    "    ).astype(int)\n",
    "    group[\"Bearish_Engulfing\"] = (\n",
    "        (prev_close > prev_open) & (close < open_) & (close < prev_open) & (open_ > prev_close)\n",
    "    ).astype(int)\n",
    "\n",
    "    if len(close) >= 10: group[\"EMA10\"] = safe_ta(ta.ema, close, length=10)\n",
    "    if len(close) >= 20: group[\"EMA20\"] = safe_ta(ta.ema, close, length=20)\n",
    "    if len(close) >= 50: group[\"EMA50\"] = safe_ta(ta.ema, close, length=50)\n",
    "    if len(close) >= 14: group[\"WMA14\"] = safe_ta(ta.wma, close, length=14)\n",
    "\n",
    "    psar = safe_ta(ta.psar, high, low, close, step=0.02, max_step=0.2)\n",
    "    if isinstance(psar, pd.DataFrame):\n",
    "        group['PSAR'] = psar.get('PSAR_0.02_0.2')\n",
    "        group['PSAR_up'] = psar.get('PSARl_0.02_0.2')\n",
    "        group['PSAR_down'] = psar.get('PSARs_0.02_0.2')\n",
    "\n",
    "    ichimoku_res = safe_ta(ta.ichimoku, high, low, close, tenkan=9, kijun=26, senkou=52)\n",
    "    if isinstance(ichimoku_res, tuple):\n",
    "        ichimoku_res_clean = [x.reset_index(drop=True) for x in ichimoku_res if x is not None and not x.empty]\n",
    "        if ichimoku_res_clean:\n",
    "            ichimoku_df = pd.concat(ichimoku_res_clean, axis=1)\n",
    "            if not ichimoku_df.empty:\n",
    "                group['Tenkan_sen'] = ichimoku_df.get('ITS_9')\n",
    "                group['Kijun_sen'] = ichimoku_df.get('IKS_26')\n",
    "                group['Senkou_Span_A'] = ichimoku_df.get('ISA_9_26')\n",
    "                group['Senkou_Span_B'] = ichimoku_df.get('ISB_26_52')\n",
    "                group['Chikou_Span'] = ichimoku_df.get('ICS_26')\n",
    "\n",
    "    if len(close) >= 20: group[\"CCI20\"] = safe_ta(ta.cci, high, low, close, length=20)\n",
    "\n",
    "    aroon = safe_ta(ta.aroon, high, low, length=25)\n",
    "    if isinstance(aroon, pd.DataFrame):\n",
    "        group['Aroon_Up'] = aroon.get('AROONU_25')\n",
    "        group['Aroon_Down'] = aroon.get('AROOND_25')\n",
    "        group['Aroon_Oscillator'] = aroon.get('AROONOSC_25')\n",
    "\n",
    "    supertrend = safe_ta(ta.supertrend, high, low, close, length=14, multiplier=3.0)\n",
    "    if isinstance(supertrend, pd.DataFrame):\n",
    "        group['Supertrend'] = supertrend.get('SUPERT_14_3.0')\n",
    "        group['Supertrend_direction'] = supertrend.get('SUPERTd_14_3.0')\n",
    "\n",
    "    return group.reset_index()\n",
    "\n",
    "def process_exchange(exchange: str, parquet_file: str):\n",
    "    print(f\"\\n--- STARTING TECHNICAL INDICATOR APPLICATION: {exchange} ---\")\n",
    "    try:\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {parquet_file} not found. Skipping {exchange} exchange.\")\n",
    "        return None\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "    df = df.sort_values([\"ticker\", \"timestamp\"])\n",
    "\n",
    "    df_ind = df.groupby(\"ticker\", group_keys=False).apply(add_all_indicators_with_patterns)\n",
    "\n",
    "    out_path = f\"{exchange.lower()}_with_indicators_patterns.parquet\"\n",
    "    df_ind.to_parquet(out_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    print(f\"Saved {out_path}, {len(df_ind)} rows, {df_ind['ticker'].nunique()} tickers\")\n",
    "    return df_ind\n",
    "\n",
    "process_exchange(\"HOSE\", \"hose_all_by1d.parquet\")\n",
    "process_exchange(\"HNX\", \"hnx_all_by1d.parquet\")\n",
    "process_exchange(\"UPCOM\", \"upcom_all_by1d.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b658358",
   "metadata": {},
   "source": [
    "### 2.2 TA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bf384f",
   "metadata": {},
   "source": [
    "### Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_USERNAME = os.getenv(\"API_USERNAME\")\n",
    "API_PASSWORD = os.getenv(\"API_PASSWORD\")\n",
    "\n",
    "INPUT_DIR = \".\"\n",
    "OUTPUT_DIR = \"final_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FILE_CONFIG = [\n",
    "    {'input': 'hose_with_indicators_patterns.parquet', 'output': 'hose_final.parquet'},\n",
    "    {'input': 'hnx_with_indicators_patterns.parquet', 'output': 'hnx_final.parquet'},\n",
    "    {'input': 'upcom_with_indicators_patterns.parquet', 'output': 'upcom_final.parquet'}\n",
    "]\n",
    "\n",
    "try:\n",
    "    print(\"Logging in...\")\n",
    "    client = fq.FiinSession(username=API_USERNAME, password=API_PASSWORD).login()\n",
    "    fa_client = client.FundamentalAnalysis()\n",
    "except Exception as e:\n",
    "    print(f\"Error during login: {e}\")\n",
    "    exit()\n",
    "\n",
    "all_tickers = set()\n",
    "for config in FILE_CONFIG:\n",
    "    file_path = os.path.join(INPUT_DIR, config['input'])\n",
    "    if os.path.exists(file_path):\n",
    "        all_tickers.update(pd.read_parquet(file_path)['ticker'].unique())\n",
    "\n",
    "unique_tickers = sorted(list(all_tickers))\n",
    "all_fa_records = []\n",
    "\n",
    "if unique_tickers:\n",
    "    ticker_batches = [unique_tickers[i:i + 100] for i in range(0, len(unique_tickers), 100)]\n",
    "    print(f\"Fetching FA data for {len(unique_tickers)} tickers in {len(ticker_batches)} batches...\")\n",
    "    \n",
    "    for i, batch in enumerate(ticker_batches):\n",
    "        try:\n",
    "            fa_raw_batch = fa_client.get_ratios(\n",
    "                tickers=batch,\n",
    "                Period=\"Quarterly\",\n",
    "                LatestYear=2025,\n",
    "                NumberOfPeriod=13,\n",
    "                Consolidated=True\n",
    "            )\n",
    "            if fa_raw_batch:\n",
    "                all_fa_records.extend(fa_raw_batch)\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"    -> Error processing batch {i+1}. Details: {e}. Skipping this batch.\")\n",
    "            continue\n",
    "else:\n",
    "    print(\"No tickers found in source files.\")\n",
    "    exit()\n",
    "\n",
    "if not all_fa_records:\n",
    "    print(\"\\nNo FA data was successfully fetched. Please check API credentials or connection.\")\n",
    "else:\n",
    "    print(f\"Successfully fetched {len(all_fa_records)} FA records.\")\n",
    "    \n",
    "    flattened_list = []\n",
    "    for record in all_fa_records:\n",
    "        flat_record = {'ticker': record.get('ticker'), 'year': record.get('year'), 'quarter': record.get('quarter')}\n",
    "        for group_values in record.values():\n",
    "            if isinstance(group_values, dict):\n",
    "                flat_record.update(group_values)\n",
    "        flattened_list.append(flat_record)\n",
    "            \n",
    "    df_fa_master = pd.DataFrame(flattened_list)\n",
    "    df_fa_master['timestamp'] = pd.to_datetime(df_fa_master['year'].astype(str) + 'Q' + df_fa_master['quarter'].astype(str))\n",
    "    \n",
    "    print(\"\\nMerging TA and FA data for each exchange...\")\n",
    "    for config in FILE_CONFIG:\n",
    "        input_file = os.path.join(INPUT_DIR, config['input'])\n",
    "        output_file = os.path.join(OUTPUT_DIR, config['output'])\n",
    "        \n",
    "        if not os.path.exists(input_file):\n",
    "            continue\n",
    "            \n",
    "        df_ta = pd.read_parquet(input_file)\n",
    "        df_ta['timestamp'] = pd.to_datetime(df_ta['timestamp'])\n",
    "        \n",
    "        df_final = pd.merge_asof(\n",
    "            df_ta.sort_values('timestamp'),\n",
    "            df_fa_master.sort_values('timestamp'),\n",
    "            on='timestamp',\n",
    "            by='ticker',\n",
    "            direction='backward'\n",
    "        )\n",
    "        \n",
    "        df_final.to_parquet(output_file, index=False)\n",
    "        print(f\"  -> Saved final file to: {output_file}\")\n",
    "\n",
    "    print(f\"\\nProcess completed! Check the results in the '{OUTPUT_DIR}' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e43af5",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac01e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_USERNAME = os.getenv(\"API_USERNAME\")\n",
    "API_PASSWORD = os.getenv(\"API_PASSWORD\")\n",
    "\n",
    "TA_INPUT_DIR = \".\"\n",
    "FINAL_OUTPUT_DIR = \"final_data\"\n",
    "os.makedirs(FINAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FILE_CONFIG = [\n",
    "    {'input': 'hose_with_indicators_patterns.parquet', 'output': 'hose_final_data.parquet'},\n",
    "    {'input': 'hnx_with_indicators_patterns.parquet', 'output': 'hnx_final_data.parquet'},\n",
    "    {'input': 'upcom_with_indicators_patterns.parquet', 'output': 'upcom_final_data.parquet'}\n",
    "]\n",
    "FA_LATEST_YEAR = 2025\n",
    "FA_NUMBER_OF_PERIODS = 13\n",
    "FA_BATCH_SIZE = 100\n",
    "\n",
    "def fetch_fa_data_robustly(fa_client, tickers):\n",
    "    \"\"\"Fetches FA data by batch, falling back to single-ticker requests if a batch fails.\"\"\"\n",
    "    all_fa_records = []\n",
    "    ticker_batches = [tickers[i:i + FA_BATCH_SIZE] for i in range(0, len(tickers), FA_BATCH_SIZE)]\n",
    "    print(f\"Fetching FA data for {len(tickers)} tickers in {len(ticker_batches)} batches...\")\n",
    "    \n",
    "    for i, batch in enumerate(ticker_batches):\n",
    "        try:\n",
    "            fa_raw_batch = fa_client.get_ratios(\n",
    "                tickers=batch, Period=\"Quarterly\", LatestYear=FA_LATEST_YEAR,\n",
    "                NumberOfPeriod=FA_NUMBER_OF_PERIODS, Consolidated=True\n",
    "            )\n",
    "            if fa_raw_batch:\n",
    "                all_fa_records.extend(fa_raw_batch)\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"    -> Batch {i+1} failed: {e}. Switching to single-ticker mode for this batch.\")\n",
    "            for ticker in batch:\n",
    "                try:\n",
    "                    fa_raw_single = fa_client.get_ratios(\n",
    "                        tickers=[ticker], Period=\"Quarterly\", LatestYear=FA_LATEST_YEAR,\n",
    "                        NumberOfPeriod=FA_NUMBER_OF_PERIODS, Consolidated=True\n",
    "                    )\n",
    "                    if fa_raw_single:\n",
    "                        all_fa_records.extend(fa_raw_single)\n",
    "                except Exception:\n",
    "                    # Silently skip invalid tickers\n",
    "                    pass\n",
    "    return all_fa_records\n",
    "\n",
    "def process_and_clean(df_ta, df_fa_master):\n",
    "    \"\"\"Merges, cleans _x/_y columns, and flattens dictionary columns in one go.\"\"\"\n",
    "    # Merge\n",
    "    df_merged = pd.merge_asof(\n",
    "        df_ta.sort_values('timestamp'),\n",
    "        df_fa_master.sort_values('timestamp'),\n",
    "        on='timestamp', by='ticker', direction='backward'\n",
    "    )\n",
    "    \n",
    "    # Clean _x, _y columns\n",
    "    cols_x = [col for col in df_merged.columns if col.endswith('_x')]\n",
    "    for col_x in cols_x:\n",
    "        base_name, col_y = col_x[:-2], f\"{col_x[:-2]}_y\"\n",
    "        if col_y in df_merged.columns:\n",
    "            df_merged[base_name] = df_merged[col_y].fillna(df_merged[col_x])\n",
    "            df_merged = df_merged.drop(columns=[col_x, col_y])\n",
    "        else:\n",
    "            df_merged = df_merged.rename(columns={col_x: base_name})\n",
    "\n",
    "    # Flatten dictionary columns\n",
    "    cols_to_flatten = [col for col in df_merged.columns if 'Ratio' in col and df_merged[col].apply(lambda x: isinstance(x, dict)).any()]\n",
    "    if not cols_to_flatten:\n",
    "        return df_merged\n",
    "        \n",
    "    df_processed = df_merged.copy()\n",
    "    for col_name in cols_to_flatten:\n",
    "        series_normalized = df_processed[col_name].apply(lambda x: x if isinstance(x, dict) else {})\n",
    "        normalized_df = pd.json_normalize(series_normalized)\n",
    "        normalized_df.index = df_processed.index\n",
    "        df_processed = pd.concat([df_processed, normalized_df], axis=1)\n",
    "    \n",
    "    return df_processed.drop(columns=cols_to_flatten)\n",
    "\n",
    "try:\n",
    "    print(\"Logging in...\")\n",
    "    client = fq.FiinSession(username=API_USERNAME, password=API_PASSWORD).login()\n",
    "    fa_client = client.FundamentalAnalysis()\n",
    "except Exception as e:\n",
    "    print(f\"Fatal error during login: {e}\"); exit()\n",
    "\n",
    "all_tickers = set()\n",
    "for config in FILE_CONFIG:\n",
    "    file_path = os.path.join(TA_INPUT_DIR, config['input'])\n",
    "    if os.path.exists(file_path):\n",
    "        all_tickers.update(pd.read_parquet(file_path)['ticker'].unique())\n",
    "unique_tickers = sorted(list(all_tickers))\n",
    "\n",
    "if not unique_tickers:\n",
    "    print(\"Error: No tickers found in source files.\"); exit()\n",
    "\n",
    "raw_fa_data = fetch_fa_data_robustly(fa_client, unique_tickers)\n",
    "\n",
    "if not raw_fa_data:\n",
    "    print(\"Error: No FA data could be fetched.\"); exit()\n",
    "    \n",
    "print(f\"\\nSuccessfully fetched {len(raw_fa_data)} FA records.\")\n",
    "flattened_list = []\n",
    "for record in raw_fa_data:\n",
    "    try:\n",
    "        flat_record = {'ticker': record.get('ticker'), 'year': record.get('year'), 'quarter': record.get('quarter')}\n",
    "        for group_values in record.values():\n",
    "            if isinstance(group_values, dict):\n",
    "                flat_record.update(group_values)\n",
    "        flattened_list.append(flat_record)\n",
    "    except Exception:\n",
    "        continue\n",
    "df_fa_master = pd.DataFrame(flattened_list)\n",
    "df_fa_master['timestamp'] = pd.to_datetime(df_fa_master['year'].astype(str) + 'Q' + df_fa_master['quarter'].astype(str))\n",
    "\n",
    "for config in FILE_CONFIG:\n",
    "    input_file = os.path.join(TA_INPUT_DIR, config['input'])\n",
    "    output_file = os.path.join(FINAL_OUTPUT_DIR, config['output'])\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nProcessing {config['input']}...\")\n",
    "    df_ta = pd.read_parquet(input_file)\n",
    "    df_ta['timestamp'] = pd.to_datetime(df_ta['timestamp'])\n",
    "    \n",
    "    df_final = process_and_clean(df_ta, df_fa_master)\n",
    "    \n",
    "    df_final.to_parquet(output_file, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    print(f\"  -> Saved final file to: {output_file}\")\n",
    "\n",
    "print(f\"\\nPipeline complete! Check results in the '{FINAL_OUTPUT_DIR}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14882726",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"final_data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "FILES_TO_PROCESS = [\n",
    "    \"hose_final.parquet\",\n",
    "    \"hnx_final.parquet\",\n",
    "    \"upcom_final.parquet\"\n",
    "]\n",
    "\n",
    "COLS_TO_FLATTEN_PREFIXES = [\n",
    "    'ProfitabilityRatio', 'GrowthRatio', 'ValuationRatios', 'LiquidityRatio',\n",
    "    'FinancialLeverageRatio', 'EfficiencyRatio', 'SolvencyRatio'\n",
    "]\n",
    "\n",
    "def parse_item(item):\n",
    "    \"\"\"Safely handles values that could be a dict, a JSON string, or other types.\"\"\"\n",
    "    if isinstance(item, dict):\n",
    "        return item\n",
    "    if isinstance(item, str):\n",
    "        try:\n",
    "            return json.loads(item.replace(\"'\", \"\\\"\"))\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "print(\"--- Starting Column Flattening Process ---\")\n",
    "\n",
    "for file_name in FILES_TO_PROCESS:\n",
    "    file_path = os.path.join(DATA_DIR, file_name)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: File not found, skipping: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing: {file_name}...\")\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    cols_to_flatten = [\n",
    "        col for col in df.columns \n",
    "        if any(col.startswith(prefix) for prefix in COLS_TO_FLATTEN_PREFIXES)\n",
    "    ]\n",
    "\n",
    "    if not cols_to_flatten:\n",
    "        print(\"  -> No specified columns to flatten found in this file.\")\n",
    "        continue\n",
    "        \n",
    "    df_processed = df.copy()\n",
    "    for col_name in cols_to_flatten:\n",
    "        normalized_df = pd.json_normalize(\n",
    "            df_processed[col_name].apply(parse_item)\n",
    "        )\n",
    "        \n",
    "        normalized_df.index = df_processed.index\n",
    "        \n",
    "        df_processed = pd.concat([df_processed, normalized_df], axis=1)\n",
    "        df_processed = df_processed.drop(columns=[col_name])\n",
    "    \n",
    "    df_processed.to_parquet(file_path, index=False)\n",
    "\n",
    "print(f\"\\nProcess complete! All files in the '{DATA_DIR}' directory have been updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f258f9",
   "metadata": {},
   "source": [
    "## 3. Apply ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ce74d",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "- Dữ liệu dùng để train là từ ngày cũ nhất trong data đến 2024-08-31\n",
    "- Dữ liệu để để validate là từ 2024-08-31 đến 2024-12-31\n",
    "- Dữ liệu để backtest là các cổ phiếu trong năm 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d9cc8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING TASK 4 (ADVANCED): BUILDING THE ADVANCED PREDICTION MODEL ---\n",
      "Reading and concatenating data from the 3 exchanges...\n",
      "Performing advanced feature engineering...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m period \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m]: group[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperiod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpct_change(period)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m---> 38\u001b[0m df_featured \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(create_advanced_features)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating labels for the data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m LOOK_FORWARD_DAYS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m; PROFIT_THRESHOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.10\u001b[39m; STOP_LOSS_THRESHOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1824\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m   1823\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1824\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1825\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[1;32m   1826\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1827\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[1;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1829\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1830\u001b[0m         ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1889\u001b[0m, in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/generic.py:1584\u001b[0m, in \u001b[0;36mDataFrameGroupBy._wrap_applied_output\u001b[0;34m(self, data, values, not_indexed_same, is_transform)\u001b[0m\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_constructor()\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_not_none, DataFrame):\n\u001b[0;32m-> 1584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concat_objects(\n\u001b[1;32m   1585\u001b[0m         values,\n\u001b[1;32m   1586\u001b[0m         not_indexed_same\u001b[38;5;241m=\u001b[39mnot_indexed_same,\n\u001b[1;32m   1587\u001b[0m         is_transform\u001b[38;5;241m=\u001b[39mis_transform,\n\u001b[1;32m   1588\u001b[0m     )\n\u001b[1;32m   1590\u001b[0m key_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mresult_index \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_not_none, (np\u001b[38;5;241m.\u001b[39mndarray, Index)):\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;66;03m# GH#1738: values is list of arrays of unequal lengths\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;66;03m#  fall through to the outer else clause\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;66;03m# TODO: sure this is right?  we used to do this\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;66;03m#  after raising AttributeError above\u001b[39;00m\n\u001b[1;32m   1597\u001b[0m     \u001b[38;5;66;03m# GH 18930\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1472\u001b[0m, in \u001b[0;36m_concat_objects\u001b[0;34m(self, values, not_indexed_same, is_transform)\u001b[0m\n\u001b[1;32m   1469\u001b[0m         keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(values)))\n\u001b[1;32m   1470\u001b[0m         result \u001b[38;5;241m=\u001b[39m concat(values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis, keys\u001b[38;5;241m=\u001b[39mkeys)\n\u001b[0;32m-> 1472\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m not_indexed_same:\n\u001b[1;32m   1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m concat(values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[1;32m   1475\u001b[0m     ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39m_get_axis(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[1;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    686\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/concat.py:157\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    154\u001b[0m     out\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m axes\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 157\u001b[0m concat_plan \u001b[38;5;241m=\u001b[39m _get_combined_plan(mgrs)\n\u001b[1;32m    159\u001b[0m blocks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    160\u001b[0m values: ArrayLike\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/concat.py:314\u001b[0m, in \u001b[0;36m_get_combined_plan\u001b[0;34m(mgrs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     blkno \u001b[38;5;241m=\u001b[39m blknos[k]\n\u001b[1;32m    313\u001b[0m     nb \u001b[38;5;241m=\u001b[39m _get_block_for_concat_plan(mgr, bp, blkno, max_len\u001b[38;5;241m=\u001b[39mmax_len)\n\u001b[0;32m--> 314\u001b[0m     unit \u001b[38;5;241m=\u001b[39m JoinUnit(nb)\n\u001b[1;32m    315\u001b[0m     units_for_bp\u001b[38;5;241m.\u001b[39mappend(unit)\n\u001b[1;32m    317\u001b[0m plan\u001b[38;5;241m.\u001b[39mappend((bp, units_for_bp))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"--- STARTING TASK 4 (ADVANCED): BUILDING THE ADVANCED PREDICTION MODEL ---\")\n",
    "\n",
    "# --- 0. CONFIGURATION & DATA PREPARATION ---\n",
    "INPUT_DIR = \"final_enriched_data_flattened\"\n",
    "OUTPUT_DIR = \"model_artifacts\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FILES_TO_PROCESS = [\n",
    "    \"hose_final_enriched_flattened.parquet\",\n",
    "    \"hnx_final_enriched_flattened.parquet\",\n",
    "    \"upcom_final_enriched_flattened.parquet\"\n",
    "]\n",
    "print(\"Reading and concatenating data from the 3 exchanges...\")\n",
    "try:\n",
    "    df = pd.concat([pd.read_parquet(os.path.join(INPUT_DIR, f)) for f in FILES_TO_PROCESS if os.path.exists(os.path.join(INPUT_DIR, f))], ignore_index=True)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values(['ticker', 'timestamp']).reset_index(drop=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading files: {e}\"); exit()\n",
    "\n",
    "print(\"Performing advanced feature engineering...\")\n",
    "feature_cols = [\n",
    "    'MA20', 'MA50', 'MA200', 'MACD_hist', 'ADX', 'RSI14', 'Stoch_K', \n",
    "    'ATR14', 'MFI', 'OBV', 'VWAP', 'Gap', 'ROE', 'PriceToEarning', \n",
    "    'ROA', 'DebtToEquity', 'Revenue_Growth', 'netProfitGrowth'\n",
    "]\n",
    "available_features = [col for col in feature_cols if col in df.columns]\n",
    "def create_advanced_features(group):\n",
    "    for col in available_features:\n",
    "        for lag in [1, 2, 3, 5]: group[f'{col}_lag_{lag}'] = group[col].shift(lag)\n",
    "    for window in [5, 10, 20]:\n",
    "        group[f'close_std_{window}'] = group['close'].rolling(window=window).std()\n",
    "        if 'RSI14' in group.columns: group[f'rsi_mean_{window}'] = group['RSI14'].rolling(window=window).mean()\n",
    "    for period in [1, 2, 3, 5, 10, 20]: group[f'return_{period}d'] = group['close'].pct_change(period)\n",
    "    return group\n",
    "df_featured = df.groupby('ticker', group_keys=False).apply(create_advanced_features)\n",
    "\n",
    "print(\"Creating labels for the data...\")\n",
    "LOOK_FORWARD_DAYS = 15; PROFIT_THRESHOLD = 0.10; STOP_LOSS_THRESHOLD = 0.05\n",
    "def create_labels(group):\n",
    "    rolling_max = group['close'].rolling(window=LOOK_FORWARD_DAYS).max().shift(-LOOK_FORWARD_DAYS)\n",
    "    rolling_min = group['close'].rolling(window=LOOK_FORWARD_DAYS).min().shift(-LOOK_FORWARD_DAYS)\n",
    "    profit_target = (rolling_max / group['close']) - 1 >= PROFIT_THRESHOLD\n",
    "    stop_loss_hit = (rolling_min / group['close']) - 1 < -STOP_LOSS_THRESHOLD\n",
    "    group['label'] = np.where(profit_target & ~stop_loss_hit, 1.0, 0.0)\n",
    "    group.loc[group['close'].shift(-LOOK_FORWARD_DAYS).isnull(), 'label'] = np.nan\n",
    "    return group\n",
    "df_labeled = df_featured.groupby('ticker', group_keys=False).apply(create_labels)\n",
    "\n",
    "print(f\"Saving processed data to '{OUTPUT_DIR}/data_for_backtest.parquet'...\")\n",
    "df_labeled.to_parquet(os.path.join(OUTPUT_DIR, 'data_for_backtest.parquet'))\n",
    "\n",
    "print(\"Preparing and cleaning data for the model...\")\n",
    "COLS_TO_DROP = ['ticker', 'timestamp', 'label', 'open', 'high', 'low', 'close']\n",
    "feature_cols_final = [col for col in df_labeled.columns if col not in COLS_TO_DROP and 'Ratio' not in col and 'Growth' != col]\n",
    "X = df_labeled[feature_cols_final].copy()\n",
    "y = df_labeled['label'].copy()\n",
    "if 'exchange' in X.columns: X = pd.get_dummies(X, columns=['exchange'], prefix='exchange', drop_first=True)\n",
    "valid_indices = y.dropna().index\n",
    "X = X.loc[valid_indices].reset_index(drop=True)\n",
    "y = y.loc[valid_indices].reset_index(drop=True)\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X.fillna(0, inplace=True)\n",
    "X.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X.columns]\n",
    "print(f\"Data preparation complete. {len(X):,} samples available for training and testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece18442",
   "metadata": {},
   "source": [
    "### PHASE 1: DISCOVERY - FINDING IMPORTANT FEATURES & BEST PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv_for_tuning = TimeSeriesSplit(n_splits=3, test_size=int(len(X)*0.1))\n",
    "train_idx, val_idx = list(tscv_for_tuning.split(X))[-1]\n",
    "X_train_tune, X_val_tune = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train_tune, y_val_tune = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "def objective(trial):\n",
    "    ratio = (y_train_tune == 0).sum() / (y_train_tune == 1).sum() if (y_train_tune == 1).sum() > 0 else 1\n",
    "    param = {\n",
    "        'objective': 'binary', 'metric': 'auc', 'verbosity': -1, 'boosting_type': 'gbdt',\n",
    "        'n_estimators': 1000, 'scale_pos_weight': ratio, 'random_state': 42, 'n_jobs': -1,\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1)\n",
    "    }\n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "    model.fit(X_train_tune, y_train_tune,\n",
    "              eval_set=[(X_val_tune, y_val_tune)],\n",
    "              eval_metric='auc',\n",
    "              callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    preds = model.predict(X_val_tune)\n",
    "    f1 = f1_score(y_val_tune, preds)\n",
    "    return f1\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "best_params = study.best_params\n",
    "print(f\"Best parameters found with F1-Score: {study.best_value:.4f}\")\n",
    "\n",
    "N_SPLITS = 5\n",
    "tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "print(f\"\\n--- [PHASE 1] Training with all features to determine importance ---\")\n",
    "feature_importances = pd.DataFrame(index=X.columns)\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    ratio = (y_train == 0).sum() / (y_train == 1).sum() if (y_train == 1).sum() > 0 else 1\n",
    "    lgb_clf = lgb.LGBMClassifier(objective='binary', metric='auc', n_estimators=1000,\n",
    "                                 scale_pos_weight=ratio, random_state=42, n_jobs=-1, **best_params)\n",
    "    lgb_clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='auc',\n",
    "                callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    feature_importances[f'fold_{i+1}'] = lgb_clf.feature_importances_\n",
    "\n",
    "feature_importances['mean'] = feature_importances.mean(axis=1)\n",
    "feature_importances.sort_values('mean', ascending=False, inplace=True)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x='mean', y=feature_importances.index[:20], data=feature_importances.head(20))\n",
    "plt.title('Top 20 Most Important Features (Discovery Phase)')\n",
    "plt.xlabel('Importance Score (Mean)')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0dcaf9",
   "metadata": {},
   "source": [
    "### PHASE 2: FINAL MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a88f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"=\"*30)\n",
    "\n",
    "TOP_N_FEATURES = 20 \n",
    "top_features = feature_importances.head(TOP_N_FEATURES).index.tolist()\n",
    "X_selected = X[top_features]\n",
    "print(f\"Selected the top {len(top_features)} features for retraining.\")\n",
    "\n",
    "original_timestamps = df_labeled.loc[valid_indices, 'timestamp'].reset_index(drop=True)\n",
    "\n",
    "TRAIN_END_DATE = pd.to_datetime('2024-08-31')\n",
    "VALIDATION_END_DATE = pd.to_datetime('2024-12-31')\n",
    "\n",
    "train_mask = original_timestamps <= TRAIN_END_DATE\n",
    "X_train_final = X_selected[train_mask]\n",
    "y_train_final = y[train_mask]\n",
    "\n",
    "validation_mask = (original_timestamps > TRAIN_END_DATE) & (original_timestamps <= VALIDATION_END_DATE)\n",
    "X_val_final = X_selected[validation_mask]\n",
    "y_val_final = y[validation_mask]\n",
    "\n",
    "print(f\"Train set: {len(X_train_final)} samples (from start to {TRAIN_END_DATE.date()})\")\n",
    "print(f\"Validation set: {len(X_val_final)} samples (from {TRAIN_END_DATE.date() + pd.Timedelta(days=1)} to {VALIDATION_END_DATE.date()})\")\n",
    "if X_val_final.empty:\n",
    "    print(\"Warning: The validation set is empty. Please check the date ranges and data.\")\n",
    "    exit()\n",
    "\n",
    "ratio = (y_train_final == 0).sum() / (y_train_final == 1).sum() if (y_train_final == 1).sum() > 0 else 1\n",
    "final_model = lgb.LGBMClassifier(\n",
    "    objective='binary', metric='auc', n_estimators=2000,\n",
    "    scale_pos_weight=ratio, random_state=42, n_jobs=-1,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "print(\"\\nStarting final model training...\")\n",
    "final_model.fit(X_train_final, y_train_final,\n",
    "                eval_set=[(X_val_final, y_val_final)],\n",
    "                eval_metric='auc',\n",
    "                callbacks=[lgb.early_stopping(150, verbose=True)])\n",
    "\n",
    "print(\"\\n--- Finding the optimal threshold on the VALIDATION set ---\")\n",
    "y_proba_val = final_model.predict_proba(X_val_final)[:, 1]\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val_final, y_proba_val)\n",
    "thresholds = np.append(thresholds, 1) \n",
    "f1_scores_curve = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "\n",
    "best_threshold_idx = np.argmax(f1_scores_curve)\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "best_f1_score = f1_scores_curve[best_threshold_idx]\n",
    "\n",
    "print(f\"Optimal threshold found: {best_threshold:.4f} with the highest F1-Score of: {best_f1_score:.4f}\")\n",
    "y_pred_best = (y_proba_val >= best_threshold).astype(int)\n",
    "print(\"\\nClassification Report (on VALIDATION set, with optimal threshold):\")\n",
    "print(classification_report(y_val_final, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6fe0da",
   "metadata": {},
   "source": [
    "### PHASE 3: SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659eb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(OUTPUT_DIR, 'final_model.pkl')\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\"Final model saved to: {model_path}\")\n",
    "\n",
    "features_path = os.path.join(OUTPUT_DIR, 'top_features.txt')\n",
    "with open(features_path, 'w') as f:\n",
    "    for feature in top_features:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "print(f\"Feature list saved to: {features_path}\")\n",
    "\n",
    "threshold_path = os.path.join(OUTPUT_DIR, 'best_threshold.txt')\n",
    "with open(threshold_path, 'w') as f:\n",
    "    f.write(str(best_threshold))\n",
    "print(f\"Optimal threshold saved to: {threshold_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8053dfdf",
   "metadata": {},
   "source": [
    "## 4. Backtest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ARTIFACTS_DIR = \"model_artifacts\"\n",
    "BACKTEST_START_DATE = '2025-01-01'\n",
    "BACKTEST_END_DATE = '2025-09-03'\n",
    "INITIAL_CAPITAL = 1_000_000_000\n",
    "MAX_POSITIONS = 5\n",
    "HOLDING_PERIOD = 15\n",
    "VNINDEX_FILE = 'hose_all_by1d.parquet'\n",
    "\n",
    "print(\"--- Loading trained artifacts ---\")\n",
    "try:\n",
    "    model = joblib.load(os.path.join(ARTIFACTS_DIR, 'final_model.pkl'))\n",
    "    \n",
    "    with open(os.path.join(ARTIFACTS_DIR, 'top_features.txt'), 'r') as f:\n",
    "        top_features = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    with open(os.path.join(ARTIFACTS_DIR, 'best_threshold.txt'), 'r') as f:\n",
    "        best_threshold = float(f.read())\n",
    "        \n",
    "    df_full = pd.read_parquet(os.path.join(ARTIFACTS_DIR, 'data_for_backtest.parquet'))\n",
    "    df_full['timestamp'] = pd.to_datetime(df_full['timestamp'])\n",
    "\n",
    "    print(\"Artifacts loaded successfully!\")\n",
    "    print(f\"  - Model: Loaded\")\n",
    "    print(f\"  - Number of features: {len(top_features)}\")\n",
    "    print(f\"  - Decision threshold: {best_threshold:.4f}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: A required file was not found. Please ensure Task 4 has been completed successfully.\")\n",
    "    print(f\"  - Detailed error: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a95c4e",
   "metadata": {},
   "source": [
    "### Logic lọc cổ phiếu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buy_signals_for_day(daily_data, model, features, threshold):\n",
    "    \"\"\"\n",
    "    Combines hard filters (Step 3) and model predictions (Step 4) to find buy signals.\n",
    "    \"\"\"\n",
    "    candidates = daily_data[daily_data['volume'] * daily_data['close'] > 5_000_000_000].copy()\n",
    "    if 'marketCap' in candidates.columns:\n",
    "        candidates = candidates[candidates['marketCap'] > 1_000_000_000_000]\n",
    "    if 'PriceToEarning' in candidates.columns:\n",
    "        candidates = candidates[candidates['PriceToEarning'] > 0]\n",
    "    \n",
    "    if candidates.empty:\n",
    "        return []\n",
    "\n",
    "    X_today = candidates[features]\n",
    "    X_today = X_today.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "    candidates['model_score'] = model.predict_proba(X_today)[:, 1]\n",
    "    \n",
    "    strong_signals = candidates[candidates['model_score'] >= threshold]\n",
    "    \n",
    "    confirmed_signals = strong_signals.copy()\n",
    "\n",
    "    if all(c in confirmed_signals.columns for c in ['MA20', 'MA50', 'MA20_lag_1', 'MA50_lag_1']):\n",
    "        golden_cross_condition = (confirmed_signals['MA20_lag_1'] <= confirmed_signals['MA50_lag_1']) & \\\n",
    "                                (confirmed_signals['MA20'] > confirmed_signals['MA50'])\n",
    "        \n",
    "        confirmed_signals = confirmed_signals[golden_cross_condition]\n",
    "\n",
    "    if confirmed_signals.empty:\n",
    "        return []\n",
    "\n",
    "    final_selection = confirmed_signals.sort_values(by='model_score', ascending=False)\n",
    "    return final_selection['ticker'].tolist()\n",
    "\n",
    "print(\"\\n--- Preparing VN-Index data for the Market Filter ---\")\n",
    "try:\n",
    "    df_vnindex_raw = pd.read_parquet(VNINDEX_FILE)\n",
    "    df_vnindex_raw['timestamp'] = pd.to_datetime(df_vnindex_raw['timestamp'])\n",
    "    \n",
    "    df_vnindex_raw = df_vnindex_raw.drop_duplicates(subset=['timestamp'], keep='first')\n",
    "    \n",
    "    df_vnindex_raw = df_vnindex_raw.sort_values(by='timestamp')\n",
    "    \n",
    "    df_vnindex_raw['MA200'] = df_vnindex_raw['close'].rolling(window=200, min_periods=1).mean()\n",
    "    \n",
    "    df_vnindex = df_vnindex_raw.set_index('timestamp')\n",
    "    print(\"VN-Index data prepared successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error preparing VN-Index data: {e}\")\n",
    "    print(\"  -> The Market Filter will be skipped, and the backtest will continue.\")\n",
    "    df_vnindex = None\n",
    "\n",
    "print(f\"\\n--- Starting backtest from {BACKTEST_START_DATE} to {BACKTEST_END_DATE} ---\")\n",
    "\n",
    "backtest_df = df_full[(df_full['timestamp'] >= BACKTEST_START_DATE) & (df_full['timestamp'] <= BACKTEST_END_DATE)]\n",
    "trade_dates = sorted(backtest_df['timestamp'].unique())\n",
    "\n",
    "cash = INITIAL_CAPITAL\n",
    "portfolio = {}\n",
    "trade_log = []\n",
    "daily_portfolio_value = []\n",
    "\n",
    "for current_date in trade_dates:\n",
    "    tickers_to_sell = []\n",
    "    for ticker, info in portfolio.items():\n",
    "        days_held = (pd.to_datetime(current_date) - info['buy_date']).days\n",
    "        if days_held >= HOLDING_PERIOD:\n",
    "            sell_price_info = backtest_df[(backtest_df['timestamp'] == current_date) & (backtest_df['ticker'] == ticker)]\n",
    "            if not sell_price_info.empty:\n",
    "                sell_price = sell_price_info['close'].iloc[0]\n",
    "                sell_value = info['quantity'] * sell_price\n",
    "                cash += sell_value\n",
    "                \n",
    "                profit = sell_value - info['invested']\n",
    "                profit_pct = (profit / info['invested']) * 100\n",
    "                trade_log.append({\n",
    "                    'ticker': ticker, 'buy_date': info['buy_date'], 'sell_date': current_date,\n",
    "                    'buy_price': info['buy_price'], 'sell_price': sell_price,\n",
    "                    'quantity': info['quantity'], 'invested': info['invested'],\n",
    "                    'sell_value': sell_value, 'profit': profit, 'profit_pct': profit_pct\n",
    "                })\n",
    "                tickers_to_sell.append(ticker)\n",
    "\n",
    "    for ticker in tickers_to_sell:\n",
    "        del portfolio[ticker]\n",
    "\n",
    "    market_is_healthy = True \n",
    "\n",
    "    if df_vnindex is not None:\n",
    "        try:\n",
    "            if current_date in df_vnindex.index:\n",
    "                vnindex_today = df_vnindex.loc[current_date]\n",
    "                \n",
    "                if isinstance(vnindex_today, pd.DataFrame):\n",
    "                    ma200_value = vnindex_today['MA200'].iloc[0]\n",
    "                    close_value = vnindex_today['close'].iloc[0]\n",
    "                else:\n",
    "                    ma200_value = vnindex_today['MA200']\n",
    "                    close_value = vnindex_today['close']\n",
    "\n",
    "                if pd.notna(ma200_value) and close_value < ma200_value:\n",
    "                    market_is_healthy = False\n",
    "        except Exception as e:\n",
    "            print(f\"  - Warning: Error checking market filter for date {current_date}: {e}\")\n",
    "            pass\n",
    "\n",
    "    if market_is_healthy:\n",
    "        positions_to_open = MAX_POSITIONS - len(portfolio)\n",
    "        if positions_to_open > 0 and cash > 0:\n",
    "            daily_snapshot = backtest_df[backtest_df['timestamp'] == current_date]\n",
    "            buy_list = get_buy_signals_for_day(daily_snapshot, model, top_features, best_threshold)\n",
    "            buy_list = [t for t in buy_list if t not in portfolio.keys()]\n",
    "            \n",
    "            if buy_list:\n",
    "                investment_per_stock = cash / positions_to_open\n",
    "                for ticker_to_buy in buy_list[:positions_to_open]:\n",
    "                    buy_price_info = daily_snapshot[daily_snapshot['ticker'] == ticker_to_buy]\n",
    "                    if not buy_price_info.empty:\n",
    "                        buy_price = buy_price_info['close'].iloc[0]\n",
    "                        if buy_price > 0 and cash >= investment_per_stock:\n",
    "                            quantity = np.floor(investment_per_stock / buy_price)\n",
    "                            invested_value = quantity * buy_price\n",
    "                            cash -= invested_value\n",
    "                            portfolio[ticker_to_buy] = {\n",
    "                                'buy_date': pd.to_datetime(current_date), 'buy_price': buy_price,\n",
    "                                'quantity': quantity, 'invested': invested_value\n",
    "                            }\n",
    "\n",
    "    current_portfolio_value = 0\n",
    "    for ticker, info in portfolio.items():\n",
    "        current_price_info = backtest_df[(backtest_df['timestamp'] == current_date) & (backtest_df['ticker'] == ticker)]\n",
    "        if not current_price_info.empty:\n",
    "            current_price = current_price_info['close'].iloc[0]\n",
    "            current_portfolio_value += info['quantity'] * current_price\n",
    "        else:\n",
    "            current_portfolio_value += info['invested']\n",
    "            \n",
    "    total_asset_value = cash + current_portfolio_value\n",
    "    daily_portfolio_value.append({'date': pd.to_datetime(current_date), 'value': total_asset_value})\n",
    "\n",
    "print(\"Backtest completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d8724",
   "metadata": {},
   "source": [
    "### Ratio and chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23533624",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"STRATEGY PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "df_log = pd.DataFrame(trade_log)\n",
    "df_value = pd.DataFrame(daily_portfolio_value).set_index('date')\n",
    "\n",
    "if df_log.empty or df_value.empty:\n",
    "    print(\"Warning: No trades were executed or no portfolio value data is available.\")\n",
    "else:\n",
    "    final_value = df_value['value'].iloc[-1]\n",
    "    total_return_pct = (final_value / INITIAL_CAPITAL - 1) * 100\n",
    "    total_trades = len(df_log)\n",
    "    win_rate = (df_log['profit'] > 0).sum() / total_trades * 100 if total_trades > 0 else 0\n",
    "    avg_profit_per_trade = df_log['profit_pct'].mean()\n",
    "    \n",
    "    print(f\"  - Results from {BACKTEST_START_DATE} to {BACKTEST_END_DATE}\")\n",
    "    print(f\"  - Total Return: {total_return_pct:.2f}%\")\n",
    "    print(f\"  - Final Portfolio Value: {final_value:,.0f} VND\")\n",
    "    print(f\"  - Total Closed Trades: {total_trades}\")\n",
    "    print(f\"  - Win Rate: {win_rate:.2f}%\")\n",
    "    print(f\"  - Average Profit per Trade: {avg_profit_per_trade:.2f}%\")\n",
    "\n",
    "    df_value['peak'] = df_value['value'].cummax()\n",
    "    df_value['drawdown'] = (df_value['value'] - df_value['peak']) / df_value['peak']\n",
    "    max_drawdown = df_value['drawdown'].min() * 100\n",
    "    print(f\"  - Maximum Drawdown: {max_drawdown:.2f}%\")\n",
    "\n",
    "    df_value['normalized_strategy'] = df_value['value'] / INITIAL_CAPITAL\n",
    "    \n",
    "    df_vnindex = None\n",
    "    try:\n",
    "        df_vnindex_raw = pd.read_parquet(VNINDEX_FILE)\n",
    "        df_vnindex_raw['timestamp'] = pd.to_datetime(df_vnindex_raw['timestamp'])\n",
    "        \n",
    "        df_vnindex_raw = df_vnindex_raw.sort_values(by='timestamp')\n",
    "        \n",
    "        df_vnindex = df_vnindex_raw.set_index('timestamp').loc[BACKTEST_START_DATE:BACKTEST_END_DATE]\n",
    "        \n",
    "        if df_vnindex.empty:\n",
    "            print(\"  - (Plotly) VN-Index data has no dates within the backtest period.\")\n",
    "            df_vnindex = None\n",
    "        else:\n",
    "            df_vnindex['normalized_vnindex'] = df_vnindex['close'] / df_vnindex['close'].iloc[0]\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"  - (Plotly) VN-Index file not found, skipping comparison.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - (Plotly) An error occurred while processing the VN-Index file: {e}\")\n",
    "        df_vnindex = None\n",
    "\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_value.index, y=df_value['normalized_strategy'], name='Q-LEAP Strategy',\n",
    "                   line=dict(color='royalblue', width=2)),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_value.index, y=df_value['peak'] / INITIAL_CAPITAL, name='Portfolio Peak',\n",
    "                   line=dict(color='rgba(0,0,0,0)')),\n",
    "        secondary_y=False\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_value.index, y=df_value['normalized_strategy'], name='Drawdown',\n",
    "                   fill='tonexty', fillcolor='rgba(255, 82, 82, 0.2)',\n",
    "                   line=dict(color='rgba(0,0,0,0)')),\n",
    "        secondary_y=False\n",
    "    )\n",
    "\n",
    "    if df_vnindex is not None:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_vnindex.index, y=df_vnindex['normalized_vnindex'], name='VN-Index',\n",
    "                       line=dict(color='red', width=1.5, dash='dash')),\n",
    "            secondary_y=False,\n",
    "        )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df_value.index, y=df_value['drawdown'] * 100, name='Drawdown (%)',\n",
    "               marker_color='crimson', opacity=0.5),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f'<b>Strategy Performance vs. VN-Index and Drawdown</b><br>Total Return: {total_return_pct:.2f}% | Max Drawdown: {max_drawdown:.2f}%',\n",
    "        xaxis_title='Date',\n",
    "        legend_title='Legend',\n",
    "        template='plotly_white',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"<b>Performance (Normalized)</b>\",\n",
    "        secondary_y=False,\n",
    "        tickformat='.2f'\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(\n",
    "        title_text=\"<b>Drawdown (%)</b>\",\n",
    "        secondary_y=True,\n",
    "        showgrid=False,\n",
    "        range=[df_value['drawdown'].min()*110, 0]\n",
    "    )\n",
    "    \n",
    "    fig.data[1].showlegend = False\n",
    "    fig.data[2].showlegend = False\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91acd8",
   "metadata": {},
   "source": [
    "### Top 10 Most Profitable Trades "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b3ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_log.sort_values('profit_pct', ascending=False).head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38db07",
   "metadata": {},
   "source": [
    "### Top 10 Biggest Losing Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_log.sort_values('profit_pct', ascending=True).head(10).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
